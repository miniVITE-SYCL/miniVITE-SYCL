************************
miniVITE-SYCL (/mini/ˈviːte/)
************************

*******
-------
 ABOUT
-------
*******
miniVITE-SYCL[*] is a proxy app that implements a single phase of Louvain 
method in distributed memory for graph community detection. It is a port 
of the original OpenMP variant (miniVITE) that is discussed in https://ieeexplore.ieee.org/abstract/document/8425242/
and whose code is hosted at: https://github.com/ECP-ExaGraph/miniVite


[*] Ghosh S, Halappanavar M, Tumeo A, Kalyanaraman 
A, Gebremedhin AH. miniVite: A graph analytics benchmarking tool 
for massively parallel systems. In 2018 IEEE/ACM Performance Modeling, 
Benchmarking and Simulation of High Performance Computer Systems 
(PMBS) 2018 Nov 12 (pp. 51-56). IEEE.

Please '*' this repository on GitHub if the code is useful to you.


******************
------------------
 SETUP ENVIRONMENT
------------------
******************

The environment is setup using the dockerfile in the repository. A `setup.sh` file
is provided that setups the environment for testing. Please execute this to setup
the environment with the OpenMP-based and SYCL-based variants.

If you do not have access to one of the repositories, please modify the `setup.sh`
script and remove downloading our modified OpenMP-based miniVITE version, and just
stick with the docker container with only the SYCL-based variant.



*************
-------------
 COMPILATION
-------------
*************

Please update the Makefile with compiler flags and use a C++20 compliant 
compiler of your choice. Invoke `make clean; make` after setting paths 
to MPI for generating the binary. Use `mpirun` or `mpiexec` or `srun`
to execute the code with specific runtime arguments mentioned in the
next section.

The following flags are available from the original program and introduced
with the new SYCL variant:

Pass -DPRINT_DIST_STATS for printing distributed graph 
characteristics.

Pass -DDEBUG_PRINTF if detailed diagonostics is required along
program run. This program requires OpenMP and C++11 support,
so pass -fopenmp (for g++)/-qopenmp (for icpc) and -std=c++11/
-std=c++0x.

Pass -DDEBUG_ASSERTIONS which will enable a debug build to root out any
semantic issues in the program via assertions

Pass -DUSE_32_BIT_GRAPH if number of nodes in the graph are 
within 32-bit range (2 x 10^9), else 64-bit range is assumed.

PASS -DSCALING_TESTS if you want to set the number of SYCL CPU threads,
which can then be set by exporting `SYCL_NUM_THREADS={count}"

PASS -DGPU_DEVICE if you want the program to create a GPU device queue
- NOTE: This is experimental and has not been validated to run on DPC++'s
CUDA backend.

Communicating vertex-community information (per iteration) 
is the most expensive step of our distributed Louvain 
implementation. We use the one of the following MPI communication 
primitives for communicating vertex-community during a Louvain
iteration, that could be enabled by passing predefined
macros at compile time:

1. MPI Collectives:  -DUSE_MPI_COLLECTIVES
2. MPI Send-Receive: -DUSE_MPI_SENDRECV
3. MPI RMA:          -DUSE_MPI_RMA (using -DUSE_MPI_ACCUMULATE 
                     additionally ensures atomic put) 
4. Default:          Uses MPI point-to-point nonblocking API in communication 
                     intensive parts..

Apart from these, we use MPI (blocking) collectives, mostly
MPI_Alltoall.

There are other predefined macros in the code as well for printing
intermediate results or checking correctness or using a particular
C++ data structure. 

***********************
-----------------------
 EXECUTING THE PROGRAM
-----------------------
***********************

The arguments for the original miniVITE program has been preserved
and should work with the SYCL version.

E.g.: 
mpiexec -n 2 bin/./miniVITE-SYCL -f karate.bin
mpiexec -n 2 bin/./miniVITE-SYCL -l -n 100
mpiexec -n 2 bin/./miniVITE-SYCL -n 100
mpiexec -n 2 bin/./miniVITE-SYCL -p 2 -n 100


Possible options (can be combined):

1. -f <bin-file>   : Specify input binary file after this argument. 
2. -b              : Only valid for real-world inputs. Attempts to distribute approximately 
                     equal number of edges among processes. Irregular number of vertices
                     owned by a particular process. Increases the distributed graph creation
                     time due to serial overheads, but may improve overall execution time.
3. -n <vertices>   : Only valid for synthetically generated inputs. Pass total number of 
                     vertices of the generated graph.
4. -l              : Use distributed LCG for randomly choosing edges. If this option 
                     is not used, we will use C++ random number generator (using 
                     std::default_random_engine).
5. -p <percent>    : Only valid for synthetically generated inputs. Specify percent of overall 
                     edges to be randomly generated between processes.
6. -t <threshold>  : Specify threshold quantity (default: 1.0E-06) used to determine the 
                     exit criteria in an iteration of Louvain method.
7. -w              : Only valid for synthetically generated inputs. Use Euclidean distance as edge weight. 
                     If this option is not used, edge weights are considered as 1.0. Generate 
                     edge weight uniformly between (0,1) if Euclidean distance is not available.                    
8. -r <nranks>     : This is used to control the number of aggregators in MPI I/O and is
                     meaningful when an input binary graph file is passed with option "-f".
                     naggr := (nranks > 1) ? (nprocs/nranks) : nranks;
9. -s              : Print graph data (edge list along with weights).
